K=8, L=1, P=12, Q=1, SE_file='data/SE1.txt', batch_size=2, d=8, decay_epoch=5, learning_rate=0.001, log_file='data/log(LondonHW)_30', max_epoch=1000, model_file='data/GMAN(LondonHW)_30', patience=10, test_ratio=0.2, time_slot=30, traffic_file='data/LondonHW_30.h5', train_ratio=0.7, val_ratio=0.1
loading data...
trainX: (12252, 12, 1000)	trainY: (12252, 1, 1000)
valX:   (1740, 12, 1000)		valY:   (1740, 1, 1000)
testX:  (3492, 12, 1000)		testY:  (3492, 1, 1000)
data loaded!
compiling model...
trainable parameters: 214,209
model compiled!
**** training model ****
2022-04-04 20:11:06 | epoch: 0001/1000, training time: 819.1s, inference time: 33.3s
train loss: 151.1641, val_loss: 208.4805
val loss decrease from inf to 208.4805, saving model to data/GMAN(LondonHW)_30
2022-04-04 20:25:12 | epoch: 0002/1000, training time: 808.7s, inference time: 32.4s
train loss: 100.2110, val_loss: 101.8341
val loss decrease from 208.4805 to 101.8341, saving model to data/GMAN(LondonHW)_30
2022-04-04 20:39:35 | epoch: 0003/1000, training time: 825.0s, inference time: 32.8s
train loss: 86.6776, val_loss: 114.0400
2022-04-04 20:53:35 | epoch: 0004/1000, training time: 807.8s, inference time: 32.5s
train loss: 80.0234, val_loss: 99.8556
val loss decrease from 101.8341 to 99.8556, saving model to data/GMAN(LondonHW)_30
2022-04-04 21:07:47 | epoch: 0005/1000, training time: 814.1s, inference time: 32.8s
train loss: 75.6320, val_loss: 88.6159
val loss decrease from 99.8556 to 88.6159, saving model to data/GMAN(LondonHW)_30
2022-04-04 21:21:50 | epoch: 0006/1000, training time: 806.1s, inference time: 32.5s
train loss: 69.3387, val_loss: 88.2831
val loss decrease from 88.6159 to 88.2831, saving model to data/GMAN(LondonHW)_30
2022-04-04 21:37:00 | epoch: 0007/1000, training time: 872.1s, inference time: 32.9s
train loss: 67.7403, val_loss: 97.9234
2022-04-04 21:51:07 | epoch: 0008/1000, training time: 814.2s, inference time: 32.5s
train loss: 66.2461, val_loss: 94.4506
2022-04-04 22:05:05 | epoch: 0009/1000, training time: 805.2s, inference time: 32.5s
train loss: 64.8481, val_loss: 85.4388
val loss decrease from 88.2831 to 85.4388, saving model to data/GMAN(LondonHW)_30
2022-04-04 22:19:12 | epoch: 0010/1000, training time: 809.2s, inference time: 32.9s
train loss: 63.4950, val_loss: 95.6298
2022-04-04 22:33:22 | epoch: 0011/1000, training time: 816.7s, inference time: 33.0s
train loss: 60.6029, val_loss: 86.9651
2022-04-04 22:47:31 | epoch: 0012/1000, training time: 815.9s, inference time: 32.9s
train loss: 59.7556, val_loss: 86.4196
2022-04-04 23:01:41 | epoch: 0013/1000, training time: 817.0s, inference time: 32.9s
train loss: 59.2531, val_loss: 89.1687
2022-04-04 23:15:51 | epoch: 0014/1000, training time: 816.9s, inference time: 32.9s
train loss: 58.6406, val_loss: 83.8874
val loss decrease from 85.4388 to 83.8874, saving model to data/GMAN(LondonHW)_30
2022-04-04 23:30:05 | epoch: 0015/1000, training time: 816.9s, inference time: 32.9s
train loss: 58.1743, val_loss: 84.2459
2022-04-04 23:44:16 | epoch: 0016/1000, training time: 816.9s, inference time: 33.0s
train loss: 56.4414, val_loss: 85.0828
2022-04-04 23:58:26 | epoch: 0017/1000, training time: 816.8s, inference time: 32.9s
train loss: 56.1768, val_loss: 83.3275
val loss decrease from 83.8874 to 83.3275, saving model to data/GMAN(LondonHW)_30
2022-04-05 00:12:40 | epoch: 0018/1000, training time: 816.7s, inference time: 33.0s
train loss: 55.8164, val_loss: 82.8197
val loss decrease from 83.3275 to 82.8197, saving model to data/GMAN(LondonHW)_30
2022-04-05 00:26:54 | epoch: 0019/1000, training time: 816.7s, inference time: 32.9s
train loss: 55.4981, val_loss: 73.1270
val loss decrease from 82.8197 to 73.1270, saving model to data/GMAN(LondonHW)_30
2022-04-05 00:41:08 | epoch: 0020/1000, training time: 816.6s, inference time: 32.9s
train loss: 55.1935, val_loss: 82.6077
2022-04-05 00:55:18 | epoch: 0021/1000, training time: 816.5s, inference time: 32.9s
train loss: 54.2243, val_loss: 77.6445
2022-04-05 01:09:28 | epoch: 0022/1000, training time: 816.5s, inference time: 32.9s
train loss: 53.9830, val_loss: 78.6981
2022-04-05 01:23:37 | epoch: 0023/1000, training time: 816.5s, inference time: 33.0s
train loss: 53.8340, val_loss: 79.6153
2022-04-05 01:37:47 | epoch: 0024/1000, training time: 816.6s, inference time: 32.9s
train loss: 53.6816, val_loss: 79.9362
2022-04-05 01:51:57 | epoch: 0025/1000, training time: 816.7s, inference time: 32.9s
train loss: 53.4436, val_loss: 81.6256
2022-04-05 02:06:07 | epoch: 0026/1000, training time: 816.5s, inference time: 33.0s
train loss: 52.9134, val_loss: 82.4203
2022-04-05 02:20:17 | epoch: 0027/1000, training time: 816.7s, inference time: 32.9s
train loss: 52.7522, val_loss: 79.8706
2022-04-05 02:34:26 | epoch: 0028/1000, training time: 816.3s, inference time: 32.9s
train loss: 52.6711, val_loss: 79.2413
2022-04-05 02:48:36 | epoch: 0029/1000, training time: 816.4s, inference time: 32.9s
train loss: 52.6016, val_loss: 81.9311
early stop at epoch: 0029
**** testing model ****
loading model from data/GMAN(LondonHW)_30
model restored!
evaluating...
testing time: 66.1s
                MAE		RMSE		MAPE
train            72.63		110.54		33.58%
val              73.13		110.92		33.08%
test             70.91		108.22		35.02%
performance in each prediction step
step: 01         70.91		108.22		35.02%
average:         70.91		108.22		35.02%
total time: 417.4min
