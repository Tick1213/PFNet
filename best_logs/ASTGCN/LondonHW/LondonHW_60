CUDA: True cuda:0
folder_dir: astgcn_r_h6d3w3_channel1_1.000000e-03
params_path: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03
create params directory experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03
param list:
CUDA	 cuda:0
in_channels	 1
nb_block	 2
nb_chev_filter	 64
nb_time_filter	 64
time_strides	 6
batch_size	 16
graph_signal_matrix_filename	 ./data/LondonHW/LondonHW_60.npz
start_epoch	 0
epochs	 400
ASTGCN_submodule(
  (BlockList): ModuleList(
    (0): ASTGCN_block(
      (TAt): Temporal_Attention_layer()
      (SAt): Spatial_Attention_layer()
      (cheb_conv_SAt): cheb_conv_withSAt(
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
        )
      )
      (time_conv): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 6), padding=(0, 1))
      (residual_conv): Conv2d(1, 64, kernel_size=(1, 1), stride=(1, 6))
      (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (1): ASTGCN_block(
      (TAt): Temporal_Attention_layer()
      (SAt): Spatial_Attention_layer()
      (cheb_conv_SAt): cheb_conv_withSAt(
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
        )
      )
      (time_conv): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
      (residual_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (final_conv): Conv2d(2, 1, kernel_size=(1, 64), stride=(1, 1))
)
Net's state_dict:
BlockList.0.TAt.U1	torch.Size([1000])
BlockList.0.TAt.U2	torch.Size([1, 1000])
BlockList.0.TAt.U3	torch.Size([1])
BlockList.0.TAt.be	torch.Size([1, 12, 12])
BlockList.0.TAt.Ve	torch.Size([12, 12])
BlockList.0.SAt.W1	torch.Size([12])
BlockList.0.SAt.W2	torch.Size([1, 12])
BlockList.0.SAt.W3	torch.Size([1])
BlockList.0.SAt.bs	torch.Size([1, 1000, 1000])
BlockList.0.SAt.Vs	torch.Size([1000, 1000])
BlockList.0.cheb_conv_SAt.Theta.0	torch.Size([1, 64])
BlockList.0.cheb_conv_SAt.Theta.1	torch.Size([1, 64])
BlockList.0.cheb_conv_SAt.Theta.2	torch.Size([1, 64])
BlockList.0.time_conv.weight	torch.Size([64, 64, 1, 3])
BlockList.0.time_conv.bias	torch.Size([64])
BlockList.0.residual_conv.weight	torch.Size([64, 1, 1, 1])
BlockList.0.residual_conv.bias	torch.Size([64])
BlockList.0.ln.weight	torch.Size([64])
BlockList.0.ln.bias	torch.Size([64])
BlockList.1.TAt.U1	torch.Size([1000])
BlockList.1.TAt.U2	torch.Size([64, 1000])
BlockList.1.TAt.U3	torch.Size([64])
BlockList.1.TAt.be	torch.Size([1, 2, 2])
BlockList.1.TAt.Ve	torch.Size([2, 2])
BlockList.1.SAt.W1	torch.Size([2])
BlockList.1.SAt.W2	torch.Size([64, 2])
BlockList.1.SAt.W3	torch.Size([64])
BlockList.1.SAt.bs	torch.Size([1, 1000, 1000])
BlockList.1.SAt.Vs	torch.Size([1000, 1000])
BlockList.1.cheb_conv_SAt.Theta.0	torch.Size([64, 64])
BlockList.1.cheb_conv_SAt.Theta.1	torch.Size([64, 64])
BlockList.1.cheb_conv_SAt.Theta.2	torch.Size([64, 64])
BlockList.1.time_conv.weight	torch.Size([64, 64, 1, 3])
BlockList.1.time_conv.bias	torch.Size([64])
BlockList.1.residual_conv.weight	torch.Size([64, 64, 1, 1])
BlockList.1.residual_conv.bias	torch.Size([64])
BlockList.1.ln.weight	torch.Size([64])
BlockList.1.ln.bias	torch.Size([64])
final_conv.weight	torch.Size([1, 2, 1, 64])
final_conv.bias	torch.Size([1])
Net's total params: 4109437
Optimizer's state_dict:
state	{}
param_groups	[{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]}]
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_0.params
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_1.params
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_2.params
global step: 1000, training loss: 1086.38, time: 365.33s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_3.params
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_4.params
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_5.params
global step: 2000, training loss: 402.65, time: 693.83s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_6.params
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_7.params
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_8.params
global step: 3000, training loss: 283.17, time: 1061.15s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_9.params
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_10.params
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_11.params
global step: 4000, training loss: 188.72, time: 1367.53s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_12.params
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_13.params
global step: 5000, training loss: 146.73, time: 1737.08s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_14.params
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_16.params
global step: 6000, training loss: 141.39, time: 2107.78s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_17.params
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_18.params
global step: 7000, training loss: 119.43, time: 2477.89s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_21.params
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_22.params
global step: 8000, training loss: 135.43, time: 2726.57s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_24.params
global step: 9000, training loss: 216.91, time: 2960.13s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_25.params
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_27.params
global step: 10000, training loss: 149.33, time: 3176.72s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_29.params
global step: 11000, training loss: 93.45, time: 3394.21s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_32.params
global step: 12000, training loss: 167.92, time: 3625.75s
global step: 13000, training loss: 161.42, time: 3839.69s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_37.params
global step: 14000, training loss: 88.01, time: 4070.41s
global step: 15000, training loss: 112.80, time: 4293.33s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_42.params
global step: 16000, training loss: 127.81, time: 4512.95s
global step: 17000, training loss: 102.77, time: 4727.82s
global step: 18000, training loss: 88.76, time: 4947.16s
global step: 19000, training loss: 79.06, time: 5166.20s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_53.params
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_55.params
global step: 20000, training loss: 121.89, time: 5385.14s
global step: 21000, training loss: 128.05, time: 5603.60s
global step: 22000, training loss: 116.12, time: 5816.66s
global step: 23000, training loss: 81.01, time: 6034.07s
global step: 24000, training loss: 137.44, time: 6258.69s
global step: 25000, training loss: 119.47, time: 6477.24s
global step: 26000, training loss: 110.75, time: 6694.37s
global step: 27000, training loss: 243.03, time: 6916.58s
global step: 28000, training loss: 99.40, time: 7138.11s
global step: 29000, training loss: 95.92, time: 7358.11s
global step: 30000, training loss: 115.91, time: 7577.77s
global step: 31000, training loss: 106.18, time: 7796.07s
global step: 32000, training loss: 99.70, time: 8013.32s
global step: 33000, training loss: 83.22, time: 8230.08s
global step: 34000, training loss: 112.69, time: 8445.60s
global step: 35000, training loss: 110.61, time: 8664.39s
global step: 36000, training loss: 85.03, time: 8882.61s
global step: 37000, training loss: 100.76, time: 9099.64s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_103.params
global step: 38000, training loss: 146.03, time: 9314.51s
global step: 39000, training loss: 114.66, time: 9533.23s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_110.params
global step: 40000, training loss: 84.35, time: 9753.15s
global step: 41000, training loss: 95.10, time: 9969.83s
global step: 42000, training loss: 101.57, time: 10189.70s
global step: 43000, training loss: 167.97, time: 10402.95s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_120.params
global step: 44000, training loss: 116.73, time: 10626.78s
global step: 45000, training loss: 90.65, time: 10846.05s
global step: 46000, training loss: 96.42, time: 11066.59s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_129.params
global step: 47000, training loss: 119.48, time: 11279.57s
global step: 48000, training loss: 75.37, time: 11496.88s
global step: 49000, training loss: 80.73, time: 11714.48s
global step: 50000, training loss: 118.11, time: 11932.96s
global step: 51000, training loss: 96.06, time: 12149.20s
global step: 52000, training loss: 94.87, time: 12367.89s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_144.params
global step: 53000, training loss: 97.06, time: 12597.74s
global step: 54000, training loss: 98.97, time: 12835.87s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_151.params
global step: 55000, training loss: 95.12, time: 13069.36s
global step: 56000, training loss: 88.11, time: 13307.06s
global step: 57000, training loss: 99.42, time: 13544.36s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_158.params
global step: 58000, training loss: 125.24, time: 13781.30s
global step: 59000, training loss: 98.51, time: 14013.85s
global step: 60000, training loss: 97.41, time: 14250.90s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_167.params
global step: 61000, training loss: 98.22, time: 14487.83s
global step: 62000, training loss: 95.78, time: 14724.90s
global step: 63000, training loss: 83.01, time: 14949.11s
global step: 64000, training loss: 102.28, time: 15162.10s
global step: 65000, training loss: 106.65, time: 15379.01s
global step: 66000, training loss: 62.71, time: 15595.79s
global step: 67000, training loss: 87.07, time: 15812.49s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_186.params
global step: 68000, training loss: 60.12, time: 16025.42s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_189.params
global step: 69000, training loss: 92.67, time: 16242.30s
global step: 70000, training loss: 95.95, time: 16459.23s
global step: 71000, training loss: 64.66, time: 16676.27s
global step: 72000, training loss: 104.19, time: 16890.43s
global step: 73000, training loss: 109.77, time: 17118.77s
global step: 74000, training loss: 96.19, time: 17356.94s
global step: 75000, training loss: 91.54, time: 17595.32s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_208.params
global step: 76000, training loss: 72.75, time: 17829.15s
global step: 77000, training loss: 79.64, time: 18067.52s
global step: 78000, training loss: 95.58, time: 18305.81s
global step: 79000, training loss: 114.35, time: 18544.11s
global step: 80000, training loss: 105.04, time: 18778.13s
global step: 81000, training loss: 83.38, time: 19016.52s
global step: 82000, training loss: 85.28, time: 19254.84s
global step: 83000, training loss: 91.81, time: 19493.29s
global step: 84000, training loss: 79.00, time: 19732.23s
global step: 85000, training loss: 89.32, time: 19965.95s
global step: 86000, training loss: 83.93, time: 20204.26s
global step: 87000, training loss: 79.01, time: 20442.58s
global step: 88000, training loss: 82.70, time: 20681.08s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_245.params
global step: 89000, training loss: 86.41, time: 20914.89s
global step: 90000, training loss: 94.70, time: 21153.12s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_250.params
global step: 91000, training loss: 73.36, time: 21391.11s
global step: 92000, training loss: 95.27, time: 21629.02s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_256.params
global step: 93000, training loss: 87.27, time: 21862.33s
global step: 94000, training loss: 88.44, time: 22100.00s
global step: 95000, training loss: 115.37, time: 22337.81s
global step: 96000, training loss: 83.41, time: 22575.52s
global step: 97000, training loss: 94.96, time: 22809.08s
global step: 98000, training loss: 87.49, time: 23046.98s
global step: 99000, training loss: 82.15, time: 23284.70s
global step: 100000, training loss: 100.05, time: 23522.50s
global step: 101000, training loss: 84.95, time: 23760.07s
global step: 102000, training loss: 88.23, time: 23993.44s
global step: 103000, training loss: 63.99, time: 24231.49s
global step: 104000, training loss: 88.32, time: 24469.52s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_289.params
global step: 105000, training loss: 140.70, time: 24707.80s
global step: 106000, training loss: 92.14, time: 24941.34s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_294.params
global step: 107000, training loss: 89.85, time: 25179.26s
global step: 108000, training loss: 78.17, time: 25417.13s
global step: 109000, training loss: 85.99, time: 25655.00s
global step: 110000, training loss: 81.67, time: 25888.47s
global step: 111000, training loss: 81.92, time: 26126.42s
global step: 112000, training loss: 81.67, time: 26344.86s
global step: 113000, training loss: 78.75, time: 26562.53s
global step: 114000, training loss: 92.19, time: 26776.57s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_315.params
global step: 115000, training loss: 73.64, time: 26997.80s
global step: 116000, training loss: 65.78, time: 27219.13s
global step: 117000, training loss: 100.99, time: 27437.98s
global step: 118000, training loss: 94.84, time: 27652.63s
global step: 119000, training loss: 104.03, time: 27871.56s
global step: 120000, training loss: 73.76, time: 28092.43s
global step: 121000, training loss: 94.07, time: 28312.68s
global step: 122000, training loss: 82.23, time: 28530.90s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_339.params
global step: 123000, training loss: 81.85, time: 28744.82s
global step: 124000, training loss: 79.34, time: 28962.27s
global step: 125000, training loss: 85.17, time: 29180.63s
global step: 126000, training loss: 94.51, time: 29402.44s
global step: 127000, training loss: 76.16, time: 29644.52s
global step: 128000, training loss: 72.96, time: 29892.13s
global step: 129000, training loss: 81.28, time: 30114.11s
global step: 130000, training loss: 80.41, time: 30416.09s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_360.params
global step: 131000, training loss: 65.27, time: 30779.43s
global step: 132000, training loss: 84.43, time: 31156.60s
global step: 133000, training loss: 88.59, time: 31446.89s
global step: 134000, training loss: 73.44, time: 31685.18s
global step: 135000, training loss: 70.68, time: 31918.79s
global step: 136000, training loss: 74.02, time: 32156.89s
global step: 137000, training loss: 64.28, time: 32394.68s
global step: 138000, training loss: 88.87, time: 32632.34s
global step: 139000, training loss: 71.73, time: 32865.53s
global step: 140000, training loss: 128.26, time: 33097.88s
save parameters to file: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_387.params
global step: 141000, training loss: 68.84, time: 33313.74s
global step: 142000, training loss: 59.86, time: 33530.30s
global step: 143000, training loss: 98.09, time: 33843.38s
global step: 144000, training loss: 71.86, time: 34211.63s
best epoch: 387
load weight from: experiments\LondonHW_60\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_387.params
predicting data set batch 1 / 104
predicting data set batch 101 / 104
input: (1652, 1000, 1, 12)
prediction: (1652, 1000, 1)
data_target_tensor: (1652, 1000, 1)
current epoch: 387, predict 0 points
MAE: 108.22
RMSE: 208.69
MAPE: 0.29
all MAE: 108.22
all RMSE: 208.69
all MAPE: 0.29
[108.219536, 208.69081, 0.28864962, 108.219536, 208.69081, 0.28864962]
