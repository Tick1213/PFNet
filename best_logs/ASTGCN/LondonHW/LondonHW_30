CUDA: True cuda:0
folder_dir: astgcn_r_h6d3w3_channel1_1.000000e-03
params_path: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03
create params directory experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03
param list:
CUDA	 cuda:0
in_channels	 1
nb_block	 2
nb_chev_filter	 64
nb_time_filter	 64
time_strides	 6
batch_size	 16
graph_signal_matrix_filename	 ./data/LondonHW/LondonHW_30.npz
start_epoch	 0
epochs	 200
ASTGCN_submodule(
  (BlockList): ModuleList(
    (0): ASTGCN_block(
      (TAt): Temporal_Attention_layer()
      (SAt): Spatial_Attention_layer()
      (cheb_conv_SAt): cheb_conv_withSAt(
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 1x64 (GPU 0)]
        )
      )
      (time_conv): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 6), padding=(0, 1))
      (residual_conv): Conv2d(1, 64, kernel_size=(1, 1), stride=(1, 6))
      (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
    (1): ASTGCN_block(
      (TAt): Temporal_Attention_layer()
      (SAt): Spatial_Attention_layer()
      (cheb_conv_SAt): cheb_conv_withSAt(
        (Theta): ParameterList(
            (0): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
            (1): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
            (2): Parameter containing: [torch.cuda.FloatTensor of size 64x64 (GPU 0)]
        )
      )
      (time_conv): Conv2d(64, 64, kernel_size=(1, 3), stride=(1, 1), padding=(0, 1))
      (residual_conv): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1))
      (ln): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (final_conv): Conv2d(2, 1, kernel_size=(1, 64), stride=(1, 1))
)
Net's state_dict:
BlockList.0.TAt.U1	torch.Size([1000])
BlockList.0.TAt.U2	torch.Size([1, 1000])
BlockList.0.TAt.U3	torch.Size([1])
BlockList.0.TAt.be	torch.Size([1, 12, 12])
BlockList.0.TAt.Ve	torch.Size([12, 12])
BlockList.0.SAt.W1	torch.Size([12])
BlockList.0.SAt.W2	torch.Size([1, 12])
BlockList.0.SAt.W3	torch.Size([1])
BlockList.0.SAt.bs	torch.Size([1, 1000, 1000])
BlockList.0.SAt.Vs	torch.Size([1000, 1000])
BlockList.0.cheb_conv_SAt.Theta.0	torch.Size([1, 64])
BlockList.0.cheb_conv_SAt.Theta.1	torch.Size([1, 64])
BlockList.0.cheb_conv_SAt.Theta.2	torch.Size([1, 64])
BlockList.0.time_conv.weight	torch.Size([64, 64, 1, 3])
BlockList.0.time_conv.bias	torch.Size([64])
BlockList.0.residual_conv.weight	torch.Size([64, 1, 1, 1])
BlockList.0.residual_conv.bias	torch.Size([64])
BlockList.0.ln.weight	torch.Size([64])
BlockList.0.ln.bias	torch.Size([64])
BlockList.1.TAt.U1	torch.Size([1000])
BlockList.1.TAt.U2	torch.Size([64, 1000])
BlockList.1.TAt.U3	torch.Size([64])
BlockList.1.TAt.be	torch.Size([1, 2, 2])
BlockList.1.TAt.Ve	torch.Size([2, 2])
BlockList.1.SAt.W1	torch.Size([2])
BlockList.1.SAt.W2	torch.Size([64, 2])
BlockList.1.SAt.W3	torch.Size([64])
BlockList.1.SAt.bs	torch.Size([1, 1000, 1000])
BlockList.1.SAt.Vs	torch.Size([1000, 1000])
BlockList.1.cheb_conv_SAt.Theta.0	torch.Size([64, 64])
BlockList.1.cheb_conv_SAt.Theta.1	torch.Size([64, 64])
BlockList.1.cheb_conv_SAt.Theta.2	torch.Size([64, 64])
BlockList.1.time_conv.weight	torch.Size([64, 64, 1, 3])
BlockList.1.time_conv.bias	torch.Size([64])
BlockList.1.residual_conv.weight	torch.Size([64, 64, 1, 1])
BlockList.1.residual_conv.bias	torch.Size([64])
BlockList.1.ln.weight	torch.Size([64])
BlockList.1.ln.bias	torch.Size([64])
final_conv.weight	torch.Size([1, 2, 1, 64])
final_conv.bias	torch.Size([1])
Net's total params: 4109437
Optimizer's state_dict:
state	{}
param_groups	[{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]}]
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_0.params
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_1.params
global step: 1000, training loss: 350.58, time: 228.46s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_2.params
global step: 2000, training loss: 115.15, time: 462.57s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_3.params
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_4.params
global step: 3000, training loss: 77.73, time: 705.45s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_5.params
global step: 4000, training loss: 56.59, time: 930.73s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_6.params
global step: 5000, training loss: 55.01, time: 1152.57s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_7.params
global step: 6000, training loss: 60.84, time: 1383.39s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_9.params
global step: 7000, training loss: 45.80, time: 1603.35s
global step: 8000, training loss: 61.69, time: 1825.05s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_12.params
global step: 9000, training loss: 64.90, time: 2037.85s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_13.params
global step: 10000, training loss: 56.55, time: 2250.64s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_15.params
global step: 11000, training loss: 58.16, time: 2472.11s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_16.params
global step: 12000, training loss: 55.19, time: 2685.37s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_17.params
global step: 13000, training loss: 39.29, time: 2900.27s
global step: 14000, training loss: 46.77, time: 3127.10s
global step: 15000, training loss: 72.93, time: 3341.90s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_21.params
global step: 16000, training loss: 46.03, time: 3566.67s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_23.params
global step: 17000, training loss: 45.73, time: 3780.32s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_24.params
global step: 18000, training loss: 50.23, time: 3994.60s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_25.params
global step: 19000, training loss: 43.56, time: 4221.41s
global step: 20000, training loss: 65.04, time: 4438.45s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_29.params
global step: 21000, training loss: 47.61, time: 4665.21s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_30.params
global step: 22000, training loss: 42.48, time: 4883.07s
global step: 23000, training loss: 41.61, time: 5102.91s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_32.params
global step: 24000, training loss: 62.61, time: 5340.35s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_34.params
global step: 25000, training loss: 48.85, time: 5557.98s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_35.params
global step: 26000, training loss: 42.04, time: 5775.51s
global step: 27000, training loss: 47.25, time: 6001.91s
global step: 28000, training loss: 48.26, time: 6219.98s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_39.params
global step: 29000, training loss: 47.80, time: 6446.41s
global step: 30000, training loss: 48.20, time: 6664.85s
global step: 31000, training loss: 51.26, time: 6882.46s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_43.params
global step: 32000, training loss: 45.96, time: 7109.65s
global step: 33000, training loss: 38.30, time: 7327.42s
global step: 34000, training loss: 47.19, time: 7553.92s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_48.params
global step: 35000, training loss: 38.45, time: 7772.36s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_49.params
global step: 36000, training loss: 46.46, time: 7990.59s
global step: 37000, training loss: 34.51, time: 8217.17s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_52.params
global step: 38000, training loss: 47.56, time: 8434.43s
global step: 39000, training loss: 37.71, time: 8659.07s
global step: 40000, training loss: 32.95, time: 8888.57s
global step: 41000, training loss: 33.55, time: 9106.62s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_57.params
global step: 42000, training loss: 32.56, time: 9332.80s
global step: 43000, training loss: 37.86, time: 9550.17s
global step: 44000, training loss: 41.60, time: 9767.20s
global step: 45000, training loss: 28.07, time: 9993.47s
global step: 46000, training loss: 37.28, time: 10211.60s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_64.params
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_65.params
global step: 47000, training loss: 32.36, time: 10435.23s
global step: 48000, training loss: 31.97, time: 10650.77s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_67.params
global step: 49000, training loss: 46.59, time: 10867.77s
global step: 50000, training loss: 33.32, time: 11093.48s
global step: 51000, training loss: 38.72, time: 11310.69s
global step: 52000, training loss: 31.51, time: 11528.29s
global step: 53000, training loss: 34.32, time: 11754.92s
global step: 54000, training loss: 39.62, time: 11972.99s
global step: 55000, training loss: 37.62, time: 12199.27s
global step: 56000, training loss: 30.49, time: 12417.98s
global step: 57000, training loss: 36.79, time: 12635.48s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_80.params
global step: 58000, training loss: 34.88, time: 12861.13s
global step: 59000, training loss: 39.72, time: 13078.10s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_82.params
global step: 60000, training loss: 41.54, time: 13296.60s
global step: 61000, training loss: 39.55, time: 13520.54s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_85.params
global step: 62000, training loss: 30.97, time: 13735.54s
global step: 63000, training loss: 39.17, time: 13983.61s
global step: 64000, training loss: 38.18, time: 14222.92s
global step: 65000, training loss: 35.54, time: 14462.46s
global step: 66000, training loss: 39.04, time: 14711.12s
global step: 67000, training loss: 34.45, time: 14950.92s
global step: 68000, training loss: 32.00, time: 15199.92s
global step: 69000, training loss: 32.44, time: 15439.19s
global step: 70000, training loss: 40.43, time: 15678.61s
global step: 71000, training loss: 33.28, time: 15927.59s
global step: 72000, training loss: 36.15, time: 16167.59s
global step: 73000, training loss: 34.13, time: 16407.30s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_102.params
global step: 74000, training loss: 32.37, time: 16656.29s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_103.params
global step: 75000, training loss: 26.14, time: 16896.03s
global step: 76000, training loss: 39.89, time: 17144.97s
global step: 77000, training loss: 36.65, time: 17384.02s
global step: 78000, training loss: 32.33, time: 17623.00s
global step: 79000, training loss: 37.55, time: 17871.38s
global step: 80000, training loss: 28.25, time: 18110.38s
global step: 81000, training loss: 34.93, time: 18358.80s
global step: 82000, training loss: 28.52, time: 18597.85s
global step: 83000, training loss: 37.40, time: 18837.18s
global step: 84000, training loss: 37.18, time: 19086.40s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_117.params
global step: 85000, training loss: 37.31, time: 19326.19s
global step: 86000, training loss: 35.61, time: 19566.55s
global step: 87000, training loss: 40.37, time: 19815.83s
global step: 88000, training loss: 34.42, time: 20055.46s
global step: 89000, training loss: 33.12, time: 20304.11s
global step: 90000, training loss: 38.22, time: 20543.65s
global step: 91000, training loss: 34.58, time: 20783.35s
global step: 92000, training loss: 35.60, time: 21032.06s
global step: 93000, training loss: 36.48, time: 21271.39s
global step: 94000, training loss: 32.85, time: 21520.03s
global step: 95000, training loss: 40.45, time: 21760.00s
global step: 96000, training loss: 35.11, time: 21999.93s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_133.params
global step: 97000, training loss: 39.09, time: 22249.34s
global step: 98000, training loss: 33.97, time: 22489.15s
global step: 99000, training loss: 39.20, time: 22729.06s
global step: 100000, training loss: 38.07, time: 22978.04s
global step: 101000, training loss: 39.42, time: 23217.54s
global step: 102000, training loss: 37.32, time: 23467.08s
global step: 103000, training loss: 26.64, time: 23707.02s
global step: 104000, training loss: 35.10, time: 23946.56s
global step: 105000, training loss: 32.99, time: 24196.01s
global step: 106000, training loss: 31.37, time: 24435.53s
global step: 107000, training loss: 32.27, time: 24675.26s
global step: 108000, training loss: 26.34, time: 24924.69s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_150.params
global step: 109000, training loss: 31.19, time: 25164.20s
global step: 110000, training loss: 38.02, time: 25413.07s
global step: 111000, training loss: 30.75, time: 25652.82s
global step: 112000, training loss: 31.31, time: 25892.66s
global step: 113000, training loss: 39.30, time: 26141.59s
global step: 114000, training loss: 30.36, time: 26381.23s
global step: 115000, training loss: 33.09, time: 26630.20s
global step: 116000, training loss: 39.74, time: 26869.56s
global step: 117000, training loss: 30.81, time: 27109.09s
global step: 118000, training loss: 31.27, time: 27358.06s
global step: 119000, training loss: 30.99, time: 27598.34s
global step: 120000, training loss: 30.08, time: 27837.40s
global step: 121000, training loss: 33.20, time: 28085.65s
global step: 122000, training loss: 24.95, time: 28324.70s
global step: 123000, training loss: 34.28, time: 28572.86s
global step: 124000, training loss: 36.92, time: 28811.79s
global step: 125000, training loss: 38.26, time: 29050.52s
global step: 126000, training loss: 28.98, time: 29298.70s
global step: 127000, training loss: 25.73, time: 29537.91s
global step: 128000, training loss: 36.05, time: 29786.48s
global step: 129000, training loss: 30.35, time: 30025.49s
global step: 130000, training loss: 29.62, time: 30264.39s
global step: 131000, training loss: 31.91, time: 30512.60s
global step: 132000, training loss: 31.49, time: 30750.98s
global step: 133000, training loss: 39.34, time: 30989.02s
global step: 134000, training loss: 34.42, time: 31236.85s
global step: 135000, training loss: 35.08, time: 31475.34s
global step: 136000, training loss: 30.10, time: 31722.97s
global step: 137000, training loss: 29.70, time: 31961.57s
global step: 138000, training loss: 29.34, time: 32200.01s
save parameters to file: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_192.params
global step: 139000, training loss: 33.69, time: 32444.23s
global step: 140000, training loss: 34.38, time: 32677.02s
global step: 141000, training loss: 32.37, time: 32918.19s
global step: 142000, training loss: 26.18, time: 33150.36s
global step: 143000, training loss: 34.41, time: 33382.82s
global step: 144000, training loss: 31.56, time: 33624.80s
best epoch: 192
load weight from: experiments\LondonHW_30\astgcn_r_h6d3w3_channel1_1.000000e-03\epoch_192.params
predicting data set batch 1 / 207
predicting data set batch 101 / 207
predicting data set batch 201 / 207
input: (3303, 1000, 1, 12)
prediction: (3303, 1000, 1)
data_target_tensor: (3303, 1000, 1)
current epoch: 192, predict 0 points
MAE: 48.38
RMSE: 92.09
MAPE: 0.26
all MAE: 48.38
all RMSE: 92.09
all MAPE: 0.26
[48.375576, 92.09415, 0.25830966, 48.375576, 92.09415, 0.25830966]
